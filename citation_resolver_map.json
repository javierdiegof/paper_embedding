{
    "Attention is All you Need": {
        "seed_id": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
        "citations_by_title": {
            "A Deep Reinforced Model for Abstractive Summarization": {
                "paperId": "032274e57f7d8b456bd255fe76b909b2c1d7458e",
                "corpusId": "21850704"
            },
            "Convolutional Sequence to Sequence Learning": {
                "paperId": "43428880d75b3a14257c3ee9bda054e61eb869c0",
                "corpusId": "3648736"
            },
            "Massive Exploration of Neural Machine Translation Architectures": {
                "paperId": "4550a4c714920ef57d19878e31c9ebae37b049b2",
                "corpusId": "2201909"
            },
            "A Structured Self-attentive Sentence Embedding": {
                "paperId": "204a4a70428f3938d2c538a4d74c7ae0416306d8",
                "corpusId": "15280949"
            },
            "Factorization tricks for LSTM networks": {
                "paperId": "79baf48bd560060549998d7b61751286de062e2a",
                "corpusId": "3570621"
            },
            "Structured Attention Networks": {
                "paperId": "13d9323a8716131911bfda048a40e2cde1a76a46",
                "corpusId": "6961760"
            },
            "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer": {
                "paperId": "510e26733aaff585d65701b9f1be7ca9d5afc586",
                "corpusId": "12462234"
            },
            "Neural Machine Translation in Linear Time": {
                "paperId": "98445f4172659ec5e891e031d8202c102135c644",
                "corpusId": "13895969"
            },
            "Can Active Memory Replace Attention?": {
                "paperId": "735d547fc75e0772d2a78c46a1cc5fad7da1474c",
                "corpusId": "11250862"
            },
            "Xception: Deep Learning with Depthwise Separable Convolutions": {
                "paperId": "5b6ec746d309b165f9f9def873a2375b6fb40f3d",
                "corpusId": "2375110"
            },
            "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation": {
                "paperId": "c6850869aa5e78a107c378d2e8bfa39633158c0c",
                "corpusId": "3603249"
            },
            "Using the Output Embedding to Improve Language Models": {
                "paperId": "63e39cdf1ad884da6bc69096bb3413b5b1100559",
                "corpusId": "836219"
            },
            "Layer Normalization": {
                "paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5",
                "corpusId": "8236317"
            },
            "Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation": {
                "paperId": "b60abe57bc195616063be10638c6437358c81d1e",
                "corpusId": "8586038"
            },
            "Exploring the Limits of Language Modeling": {
                "paperId": "2f2d8f8072e5cc9b296fad551f65f183bdbff7aa",
                "corpusId": "260422"
            },
            "Recurrent Neural Network Grammars": {
                "paperId": "7345843e87c81e24e42264859b214d26042f8d51",
                "corpusId": "1949831"
            },
            "Long Short-Term Memory-Networks for Machine Reading": {
                "paperId": "13fe71da009484f240c46f14d9330e932f8de210",
                "corpusId": "6506243"
            },
            "Deep Residual Learning for Image Recognition": {
                "paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
                "corpusId": "206594692"
            },
            "Rethinking the Inception Architecture for Computer Vision": {
                "paperId": "23ffaa0fe06eae05817f527a47ac3291077f9e58",
                "corpusId": "206593880"
            },
            "Neural GPUs Learn Algorithms": {
                "paperId": "5e4eb58d5b47ac1c73f4cf189497170e75ae6237",
                "corpusId": "2009318"
            },
            "Multi-task Sequence to Sequence Learning": {
                "paperId": "d76c07211479e233f7c6a6f32d5346c983c5598f",
                "corpusId": "6954272"
            },
            "Neural Machine Translation of Rare Words with Subword Units": {
                "paperId": "1518039b5001f1836565215eb047526b3ac7f462",
                "corpusId": "1114678"
            },
            "Effective Approaches to Attention-based Neural Machine Translation": {
                "paperId": "93499a7c7f699b6630a86fad964536f9423bb6d0",
                "corpusId": "1998416"
            },
            "End-To-End Memory Networks": {
                "paperId": "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e",
                "corpusId": "1399322"
            },
            "Grammar as a Foreign Language": {
                "paperId": "47570e7f63e296f224a0e7f9a0d08b0de3cbaf40",
                "corpusId": "14223"
            },
            "Adam: A Method for Stochastic Optimization": {
                "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
                "corpusId": "6628106"
            },
            "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling": {
                "paperId": "ac3ee98020251797c2b401e1389461df88e52e62",
                "corpusId": "5201925"
            },
            "Sequence to Sequence Learning with Neural Networks": {
                "paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d",
                "corpusId": "7961699"
            },
            "Neural Machine Translation by Jointly Learning to Align and Translate": {
                "paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5",
                "corpusId": "11212020"
            },
            "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation": {
                "paperId": "0b544dfe355a5070b60986319a3f51fb45d1348e",
                "corpusId": "5590763"
            },
            "Generating Sequences With Recurrent Neural Networks": {
                "paperId": "6471fd1cbc081fb3b7b5b14d6ab9eaaba02b5c17",
                "corpusId": "1697424"
            },
            "Fast and Accurate Shift-Reduce Constituent Parsing": {
                "paperId": "174bbdb96252454cbb40a9c4e53335996235a008",
                "corpusId": "10361562"
            },
            "Self-Training PCFG Grammars with Latent Annotations Across Languages": {
                "paperId": "5bfd8d40bc071fffaf93685a46974b122ee4239d",
                "corpusId": "447315"
            },
            "Learning Accurate, Compact, and Interpretable Tree Annotation": {
                "paperId": "f52de7242e574b70410ca6fb70b79c811919fc00",
                "corpusId": "6684426"
            },
            "Effective Self-Training for Parsing": {
                "paperId": "78a9513e70f596077179101f6cb6eadc51602039",
                "corpusId": "628455"
            },
            "Long Short-Term Memory": {
                "paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10",
                "corpusId": "1915014"
            },
            "Building a Large Annotated Corpus of English: The Penn Treebank": {
                "paperId": "0b44fcbeea9415d400c5f5789d6b892b6f98daff",
                "corpusId": "252796"
            },
            "Dropout: a simple way to prevent neural networks from overfitting": {
                "paperId": "34f25a8704614163c4095b3ee2fc969b60de4698",
                "corpusId": "6844431"
            },
            "Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies": {
                "paperId": "2e5f2b57f4c476dd69dc22ccdf547e48f40a994c",
                "corpusId": "17278462"
            }
        }
    },
    "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding": {
        "seed_id": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
        "citations_by_title": {
            "Multi-Granularity Hierarchical Attention Fusion Networks for Reading Comprehension and Question Answering": {
                "paperId": "26b47e35fe6e4260fdf7b7cc98f279a73c277494",
                "corpusId": "51867574"
            },
            "U-Net: Machine Reading Comprehension with Unanswerable Questions": {
                "paperId": "27e98e09cf09bc13c913d01676e5f32624011050",
                "corpusId": "53116060"
            },
            "Semi-Supervised Sequence Modeling with Cross-View Training": {
                "paperId": "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e",
                "corpusId": "52811641"
            },
            "Dissecting Contextual Word Embeddings: Architecture and Representation": {
                "paperId": "ac11062f1f368d97f4c826c317bf50dcc13fdb59",
                "corpusId": "52098907"
            },
            "SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference": {
                "paperId": "af5c4b80fbf847f69a202ba5a780a3dd18c1a027",
                "corpusId": "52019251"
            },
            "Character-Level Language Modeling with Deeper Self-Attention": {
                "paperId": "b9de9599d7241459db9213b5cdd7059696f5ef8d",
                "corpusId": "52004855"
            },
            "Contextual String Embeddings for Sequence Labeling": {
                "paperId": "421fc2556836a6b441de806d7b393a35b6eaea58",
                "corpusId": "52010710"
            },
            "Neural Network Acceptability Judgments": {
                "paperId": "cb0f3ee1e98faf92429d601cdcd76c69c1e484eb",
                "corpusId": "44072099"
            },
            "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding": {
                "paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c",
                "corpusId": "5034059"
            },
            "Deep Contextualized Word Representations": {
                "paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f",
                "corpusId": "3626819"
            },
            "An efficient framework for learning sentence representations": {
                "paperId": "ad31866da7f14ae21bd38df0a3b1ffd1a1438122",
                "corpusId": "3525802"
            },
            "QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension": {
                "paperId": "8c1b00128e74f1cd92aede3959690615695d5101",
                "corpusId": "4842909"
            },
            "MaskGAN: Better Text Generation via Filling in the ______": {
                "paperId": "7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d",
                "corpusId": "3655946"
            },
            "Universal Language Model Fine-tuning for Text Classification": {
                "paperId": "1e077413b25c4d34945cc2707e17e46ed4fe784a",
                "corpusId": "40100965"
            },
            "Simple and Effective Multi-Paragraph Reading Comprehension": {
                "paperId": "3c78c6df5eb1695b6a399e346dde880af27d1016",
                "corpusId": "223637"
            },
            "Learned in Translation: Contextualized Word Vectors": {
                "paperId": "bc8fa64625d9189f5801837e7b133e7fe3c581f7",
                "corpusId": "9447219"
            },
            "SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation": {
                "paperId": "a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096",
                "corpusId": "4421747"
            },
            "Attention is All you Need": {
                "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
                "corpusId": "13756489"
            },
            "Reinforced Mnemonic Reader for Machine Reading Comprehension": {
                "paperId": "e0222a1ae6874f7fff128c3da8769ab95963da04",
                "corpusId": "13559921"
            },
            "Supervised Learning of Universal Sentence Representations from Natural Language Inference Data": {
                "paperId": "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c",
                "corpusId": "28971531"
            },
            "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension": {
                "paperId": "f010affab57b5fcf1cd6be23df79d8ec98c7289c",
                "corpusId": "26501419"
            },
            "Discourse-Based Objectives for Fast Unsupervised Sentence Representation Learning": {
                "paperId": "a97dc52807d80454e78d255f9fbd7b0fab56bd03",
                "corpusId": "6694822"
            },
            "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference": {
                "paperId": "5ded2b8c64491b4a67f6d39ce473d4b9347a672e",
                "corpusId": "3432876"
            },
            "Semi-supervised sequence tagging with bidirectional language models": {
                "paperId": "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38",
                "corpusId": "7197241"
            },
            "Bidirectional Attention Flow for Machine Comprehension": {
                "paperId": "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4",
                "corpusId": "8535316"
            },
            "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation": {
                "paperId": "c6850869aa5e78a107c378d2e8bfa39633158c0c",
                "corpusId": "3603249"
            },
            "context2vec: Learning Generic Context Embedding with Bidirectional LSTM": {
                "paperId": "59761abc736397539bdd01ad7f9d91c8607c0457",
                "corpusId": "7890036"
            },
            "Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units": {
                "paperId": "4361e64f2d12d63476fdc88faf72a0f70d9a2ffb",
                "corpusId": "2359786"
            },
            "SQuAD: 100,000+ Questions for Machine Comprehension of Text": {
                "paperId": "05dd7254b632376973f3a1b4d39485da17814df5",
                "corpusId": "11816014"
            },
            "A Decomposable Attention Model for Natural Language Inference": {
                "paperId": "2cd8e8f510c89c7c18268e8ad51c061e459ad321",
                "corpusId": "8495258"
            },
            "Learning Distributed Representations of Sentences from Unlabelled Data": {
                "paperId": "26e743d5bd465f49b9538deaf116c15e61b7951f",
                "corpusId": "2937095"
            },
            "Semi-supervised Sequence Learning": {
                "paperId": "4aa9f5150b46320f534de4747a2dd0cd7f3fe292",
                "corpusId": "7138078"
            },
            "A large annotated corpus for learning natural language inference": {
                "paperId": "f04df4e20a18358ea2f689b4c129781628ef7fc1",
                "corpusId": "14604520"
            },
            "Skip-Thought Vectors": {
                "paperId": "6e795c6e9916174ae12349f5dc3f516570c17ce8",
                "corpusId": "9126867"
            },
            "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books": {
                "paperId": "0e6824e137847be0599bb0032e37042ed2ef5045",
                "corpusId": "6866988"
            },
            "How transferable are features in deep neural networks?": {
                "paperId": "081651b38ff7533550a3adfc1c00da333a8fe86c",
                "corpusId": "362467"
            },
            "GloVe: Global Vectors for Word Representation": {
                "paperId": "f37e1b62a767a307c046404ca96bc140b3e68cb5",
                "corpusId": "1957433"
            },
            "Distributed Representations of Sentences and Documents": {
                "paperId": "f3de86aeb442216a8391befcacb49e58b478f512",
                "corpusId": "2407601"
            },
            "One billion word benchmark for measuring progress in statistical language modeling": {
                "paperId": "5d833331b0e22ff359db05c62a8bca18c4f04b68",
                "corpusId": "14136307"
            },
            "Distributed Representations of Words and Phrases and their Compositionality": {
                "paperId": "87f40e6f3022adbc1f1905e3e506abad05a9964f",
                "corpusId": "16447573"
            },
            "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank": {
                "paperId": "687bac2d3320083eb4530bf18bb8f8f721477600",
                "corpusId": "990233"
            },
            "The Winograd Schema Challenge": {
                "paperId": "128cb6b891aee1b5df099acb48e2efecfcff689f",
                "corpusId": "15710851"
            },
            "Word Representations: A Simple and General Method for Semi-Supervised Learning": {
                "paperId": "8492070dc4031ed825e95e4803781752bb5e909f",
                "corpusId": "629094"
            },
            "Natural Language Understanding": {
                "paperId": "7be3afdb7b7894321027ec90ea0a990aa7a0f266",
                "corpusId": "28817116"
            },
            "ImageNet: A large-scale hierarchical image database": {
                "paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e",
                "corpusId": "57246310"
            },
            "A Scalable Hierarchical Distributed Language Model": {
                "paperId": "a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb",
                "corpusId": "10097073"
            },
            "A unified architecture for natural language processing: deep neural networks with multitask learning": {
                "paperId": "57458bc1cffe5caa45a885af986d70f723f406b4",
                "corpusId": "2617020"
            },
            "Extracting and composing robust features with denoising autoencoders": {
                "paperId": "843959ffdccf31c6694d135fad07425924f785b1",
                "corpusId": "207168299"
            },
            "Domain Adaptation with Structural Correspondence Learning": {
                "paperId": "9fa8d73e572c3ca824a04a5f551b602a17831bc5",
                "corpusId": "15978939"
            },
            "A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data": {
                "paperId": "2c5135a0531bc5ad7dd890f018e67a40529f5bcb",
                "corpusId": "13650160"
            },
            "Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition": {
                "paperId": "10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb",
                "corpusId": "2470716"
            },
            "Class-Based n-gram Models of Natural Language": {
                "paperId": "3de5d40b60742e3dfa86b19e7f660962298492af",
                "corpusId": "10986188"
            },
            "\u201cCloze Procedure\u201d: A New Tool for Measuring Readability": {
                "paperId": "766ce989b8b8b984f7a4691fd8c9af4bdb2b74cd",
                "corpusId": "206666846"
            },
            "Quora Question Pairs": {
                "paperId": "8ff46c88964a36985f2b45933a3d47b81bd87bd0",
                "corpusId": "233225749"
            },
            "The Seventh PASCAL Recognizing Textual Entailment Challenge": {
                "paperId": "0f8468de03ee9f12d693237bec87916311bf1c24",
                "corpusId": "5791809"
            },
            "The Sixth PASCAL Recognizing Textual Entailment Challenge": {
                "paperId": "db8885a0037fe47d973ade79d696586453710233",
                "corpusId": "858065"
            },
            "The Fourth PASCAL Recognizing Textual Entailment Challenge": {
                "paperId": "351ec42df2b60c6042addf96e6b98673bbaf4dfd",
                "corpusId": "12381965"
            },
            "Automatically Constructing a Corpus of Sentential Paraphrases": {
                "paperId": "475354f10798f110d34792b6d88f31d6d5cb099e",
                "corpusId": "16639476"
            },
            "The PASCAL Recognising Textual Entailment Challenge": {
                "paperId": "e808f28d411a958c5db81ceb111beb2638698f47",
                "corpusId": "8587959"
            }
        }
    },
    "Adam: A Method for Stochastic Optimization": {
        "seed_id": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
        "citations_by_title": {
            "Auto-Encoding Variational Bayes": {
                "paperId": "5f5dc5b9a2ba710937e2c413b37b053cd673df02",
                "corpusId": "216078090"
            },
            "Fast large-scale optimization by unifying stochastic gradient and quasi-Newton methods": {
                "paperId": "af0ee019dcc1fe7eab918e3c670a6c47e48d17f6",
                "corpusId": "10978620"
            },
            "Generating Sequences With Recurrent Neural Networks": {
                "paperId": "6471fd1cbc081fb3b7b5b14d6ab9eaaba02b5c17",
                "corpusId": "1697424"
            },
            "Fast dropout training": {
                "paperId": "ec92efde21707ddf4b81f301cd58e2051c1a2443",
                "corpusId": "10357959"
            },
            "On the importance of initialization and momentum in deep learning": {
                "paperId": "aa7bfd2304201afbb19971ebde87b17e40242e91",
                "corpusId": "10940950"
            },
            "Recent advances in deep learning for speech research at Microsoft": {
                "paperId": "6bdccfe195bc49d218acc5be750aa49e41f408e4",
                "corpusId": "13412186"
            },
            "Speech recognition with deep recurrent neural networks": {
                "paperId": "4177ec52d1b80ed57f2e72b0f9a42365f1a8598d",
                "corpusId": "206741496"
            },
            "Revisiting Natural Gradient for Deep Networks": {
                "paperId": "e8f95ccfd13689f672c39dca3eccf1c484533bcc",
                "corpusId": "15085443"
            },
            "ADADELTA: An Adaptive Learning Rate Method": {
                "paperId": "8729441d734782c3ed532a7d2d9611b438c0a09a",
                "corpusId": "7365802"
            },
            "ImageNet classification with deep convolutional neural networks": {
                "paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
                "corpusId": "195908774"
            },
            "Improving neural networks by preventing co-adaptation of feature detectors": {
                "paperId": "0060745e006c5f14ec326904119dca19c6545e51",
                "corpusId": "14832074"
            },
            "No more pesky learning rates": {
                "paperId": "e5a685f40338f9c2f3e68e142efa217aad16dd56",
                "corpusId": "5886221"
            },
            "Non-Asymptotic Analysis of Stochastic Approximation Algorithms for Machine Learning": {
                "paperId": "7658cecad68afc970f18cadbf6390439b17def87",
                "corpusId": "3806935"
            },
            "Learning Word Vectors for Sentiment Analysis": {
                "paperId": "1c61f9ef06fe74505775a833ff849185757199e7",
                "corpusId": "1428702"
            },
            "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization": {
                "paperId": "413c1142de9d91804d6d11c67ff3fed59c9fc279",
                "corpusId": "538820"
            },
            "A fast natural Newton method": {
                "paperId": "f7cc843c318d8862357485488971b26527ef1a8e",
                "corpusId": "2546945"
            },
            "Online Convex Programming and Generalized Infinitesimal Gradient Ascent": {
                "paperId": "e1f153c6df86d1ca8ecb9561daddfe7a54f901e7",
                "corpusId": "553962"
            },
            "Natural Gradient Works Efficiently in Learning": {
                "paperId": "5a767a341364de1f75bea85e0b12ba7d3586a461",
                "corpusId": "207585383"
            },
            "Acceleration of stochastic approximation by averaging": {
                "paperId": "6dc61f37ecc552413606d8c89ffbc46ec98ed887",
                "corpusId": "3548228"
            },
            "Efficient Estimations from a Slowly Convergent Robbins-Monro Process": {
                "paperId": "2991f9bb677b71c33945e89ac0c7dcf7a36fa198",
                "corpusId": "108279905"
            },
            "FUNDAMENTAL TECHNOLOGIES IN MODERN SPEECH RECOGNITION Digital Object Identifier 10.1109/MSP.2012.2205597": {
                "paperId": "31868290adf1c000c611dfc966b514d5a34e8d23",
                "corpusId": "58623492"
            },
            "Reducing the Dimensionality of Data with Neural": {
                "paperId": "c50dca78e97e335d362d6b991ae0e1448914e9a3",
                "corpusId": "14820265"
            },
            "Supporting Online Material for Reducing the Dimensionality of Data with Neural Networks": {
                "paperId": "02552a8b40f3a82a5353f596264db71d899a9b4a",
                "corpusId": "262637400"
            }
        }
    },
    "Layer Normalization": {
        "seed_id": "97fb4e3d45bb098e27e0071448b6152217bd35a5",
        "citations_by_title": {
            "Theano: A Python framework for fast computation of mathematical expressions": {
                "paperId": "6b570069f14c7588e066f7138e1f21af59d62e61",
                "corpusId": "8993325"
            },
            "Recurrent Batch Normalization": {
                "paperId": "952454718139dba3aafc6b3b67c4f514ac3964af",
                "corpusId": "1107124"
            },
            "Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks": {
                "paperId": "3d2c6941a9b4608ba52b328369a3352db2092ae0",
                "corpusId": "151231"
            },
            "Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin": {
                "paperId": "13497bd108d4412d02050e646235f456568cf822",
                "corpusId": "11590585"
            },
            "Order-Embeddings of Images and Language": {
                "paperId": "46b8cbcdff87b842c2c1d4a003c831f845096ba7",
                "corpusId": "11440692"
            },
            "Learning Deep Structure-Preserving Image-Text Embeddings": {
                "paperId": "b27e791e843c924ef052981b79490ab59fc0433d",
                "corpusId": "9059202"
            },
            "Batch normalized recurrent neural networks": {
                "paperId": "f95adc1d8daaa07a0c956826ec274ca9e2515ddc",
                "corpusId": "516518"
            },
            "Skip-Thought Vectors": {
                "paperId": "6e795c6e9916174ae12349f5dc3f516570c17ce8",
                "corpusId": "9126867"
            },
            "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books": {
                "paperId": "0e6824e137847be0599bb0032e37042ed2ef5045",
                "corpusId": "6866988"
            },
            "Teaching Machines to Read and Comprehend": {
                "paperId": "d1505c6123c102e53eb19dff312cb25cea840b72",
                "corpusId": "6203757"
            },
            "Path-SGD: Path-Normalized Optimization in Deep Neural Networks": {
                "paperId": "6fe02ad979baad659f04c3376a77dbb2cb4699a5",
                "corpusId": "2101905"
            },
            "DRAW: A Recurrent Neural Network For Image Generation": {
                "paperId": "a2785f66c20fbdf30ec26c0931584c6d6a0f4fca",
                "corpusId": "1930231"
            },
            "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift": {
                "paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e",
                "corpusId": "5808102"
            },
            "Adam: A Method for Stochastic Optimization": {
                "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
                "corpusId": "6628106"
            },
            "Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models": {
                "paperId": "2e36ea91a3c8fbff92be2989325531b4002e2afc",
                "corpusId": "7732372"
            },
            "Sequence to Sequence Learning with Neural Networks": {
                "paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d",
                "corpusId": "7961699"
            },
            "Very Deep Convolutional Networks for Large-Scale Image Recognition": {
                "paperId": "eb42cf88027de515750f230b23b1a057dc782108",
                "corpusId": "14124313"
            },
            "SemEval-2014 Task 1: Evaluation of Compositional Distributional Semantic Models on Full Sentences through Semantic Relatedness and Textual Entailment": {
                "paperId": "11ec56898a9e7f401a2affe776b5297bd4e25025",
                "corpusId": "16404002"
            },
            "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation": {
                "paperId": "0b544dfe355a5070b60986319a3f51fb45d1348e",
                "corpusId": "5590763"
            },
            "Microsoft COCO: Common Objects in Context": {
                "paperId": "71b7178df5d2b112d07e45038cb5637208659ff7",
                "corpusId": "14113767"
            },
            "Generating Sequences With Recurrent Neural Networks": {
                "paperId": "6471fd1cbc081fb3b7b5b14d6ab9eaaba02b5c17",
                "corpusId": "1697424"
            },
            "Efficient Estimation of Word Representations in Vector Space": {
                "paperId": "f6b51c8753a871dc94ff32152c00c01e94f90f09",
                "corpusId": "5959482"
            },
            "Large Scale Distributed Deep Networks": {
                "paperId": "3127190433230b3dc1abd0680bb58dced4bcd90e",
                "corpusId": "372467"
            },
            "ImageNet classification with deep convolutional neural networks": {
                "paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
                "corpusId": "195908774"
            },
            "IAM-OnDB - an on-line English sentence database acquired from handwritten text on a whiteboard": {
                "paperId": "9eb7daa88879f283ae05e359d6c502a320b833c9",
                "corpusId": "1745101"
            },
            "Seeing Stars: Exploiting Class Relationships for Sentiment Categorization with Respect to Rating Scales": {
                "paperId": "6af58c061f2e4f130c3b795c21ff0c7e3903278f",
                "corpusId": "3264224"
            },
            "Mining and summarizing customer reviews": {
                "paperId": "cdcf7cb29f37ac0546961ea8a076075b9cc1f992",
                "corpusId": "207155218"
            },
            "A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts": {
                "paperId": "167e1359943b96b9e92ee73db1df69a1f65d731d",
                "corpusId": "388"
            },
            "Natural Gradient Works Efficiently in Learning": {
                "paperId": "5a767a341364de1f75bea85e0b12ba7d3586a461",
                "corpusId": "207585383"
            },
            "FUNDAMENTAL TECHNOLOGIES IN MODERN SPEECH RECOGNITION Digital Object Identifier 10.1109/MSP.2012.2205597": {
                "paperId": "31868290adf1c000c611dfc966b514d5a34e8d23",
                "corpusId": "58623492"
            },
            "The Neural Autoregressive Distribution Estimator": {
                "paperId": "b893e7053c9c7e266a23fb13a42261a88f650210",
                "corpusId": "141054998"
            },
            "Annotating Expressions of Opinions and Emotions in Language": {
                "paperId": "1cff7cc15555c38607016aaba24059e76b160adb",
                "corpusId": "382842"
            }
        }
    }
}